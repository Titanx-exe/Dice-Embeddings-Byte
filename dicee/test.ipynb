{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5488532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "135\n",
      "Item 0: ('acquired_abnormality', (310,), (310, 963, 963, 963, 963, 963, 963, 963, 963, 963))\n",
      "Item 1: ('activity', (243,), (243, 963, 963, 963, 963, 963, 963, 963, 963, 963))\n",
      "Item 2: ('age_group', (717,), (717, 963, 963, 963, 963, 963, 963, 963, 963, 963))\n",
      "Item 3: ('alga', (623,), (623, 963, 963, 963, 963, 963, 963, 963, 963, 963))\n",
      "Item 4: ('amino_acid_peptide_or_protein', (602,), (602, 963, 963, 963, 963, 963, 963, 963, 963, 963))\n",
      "\n",
      "Example entries (str_ent, bpe_tokens, shaped_bpe):\n",
      "Entity: acquired_abnormality\n",
      "BPE tokens: (310,)\n",
      "Shaped BPE: (310, 963, 963, 963, 963, 963, 963, 963, 963, 963)\n",
      "---\n",
      "Entity: activity\n",
      "BPE tokens: (243,)\n",
      "Shaped BPE: (243, 963, 963, 963, 963, 963, 963, 963, 963, 963)\n",
      "---\n",
      "Entity: age_group\n",
      "BPE tokens: (717,)\n",
      "Shaped BPE: (717, 963, 963, 963, 963, 963, 963, 963, 963, 963)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Path to the file\n",
    "file_path = r\"C:\\Users\\Harshit Purohit\\Byte\\myenv7\\Lib\\site-packages\\dicee\\Experiments\\2025-07-04 00-13-29.116565\\ordered_bpe_entities.p\"\n",
    "\n",
    "# Open the file in binary read mode\n",
    "with open(file_path, 'rb') as f:\n",
    "    # Load the pickle data\n",
    "    ordered_bpe_entities = pickle.load(f)\n",
    "\n",
    "# Now you can examine the data\n",
    "print(type(ordered_bpe_entities))  # Check what type of data it is\n",
    "print(len(ordered_bpe_entities))   # If it's a list or dict, check its size\n",
    "\n",
    "# If it's a list, print a few examples\n",
    "if isinstance(ordered_bpe_entities, list):\n",
    "    for i, item in enumerate(ordered_bpe_entities[:5]):  # Print first 5 items\n",
    "        print(f\"Item {i}: {item}\")\n",
    "        \n",
    "# If it's a more complex structure, you might need to explore it differently\n",
    "# For example, if each item is a tuple with (str_ent, bpe_ent, shaped_bpe_ent)\n",
    "if isinstance(ordered_bpe_entities, list) and len(ordered_bpe_entities) > 0:\n",
    "    if isinstance(ordered_bpe_entities[0], tuple) and len(ordered_bpe_entities[0]) == 3:\n",
    "        print(\"\\nExample entries (str_ent, bpe_tokens, shaped_bpe):\")\n",
    "        for item in ordered_bpe_entities[:3]:\n",
    "            print(f\"Entity: {item[0]}\")\n",
    "            print(f\"BPE tokens: {item[1]}\")\n",
    "            print(f\"Shaped BPE: {item[2]}\")\n",
    "            print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95132cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entities: 135\n",
      "Data type: <class 'list'>\n",
      "\n",
      "First 20 entities:\n",
      "   entity  index\n",
      "0              0\n",
      "1              1\n",
      "2              2\n",
      "3              3\n",
      "4              4\n",
      "5              5\n",
      "6              6\n",
      "7              7\n",
      "8              8\n",
      "9              9\n",
      "10            10\n",
      "11            11\n",
      "12            12\n",
      "13            13\n",
      "14            14\n",
      "15            15\n",
      "16            16\n",
      "17            17\n",
      "18            18\n",
      "19            19\n",
      "\n",
      "Searching for specific entities:\n",
      "Entity 'concept_date_n2000' not found in the mapping\n",
      "Entity 'concept_profession_associate' not found in the mapping\n",
      "\n",
      "All entities saved to 'all_entities.csv'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Path to the file\n",
    "file_path = r\"C:\\Users\\Harshit Purohit\\Byte\\myenv7\\Lib\\site-packages\\dicee\\Experiments\\2025-08-23 01-44-03.616373\\ordered_bpe_entities.p\"\n",
    "\n",
    "# Open the file in binary read mode\n",
    "with open(file_path, 'rb') as f:\n",
    "    # Load the pickle data\n",
    "    ordered_bpe_entities = pickle.load(f)\n",
    "\n",
    "# Basic information\n",
    "print(f\"Total entities: {len(ordered_bpe_entities)}\")\n",
    "print(f\"Data type: {type(ordered_bpe_entities)}\")\n",
    "\n",
    "# Create a mapping dictionary (entity string → index)\n",
    "entity_to_idx = {item[0]: idx for idx, item in enumerate(ordered_bpe_entities)}\n",
    "\n",
    "# Create a DataFrame for easier analysis\n",
    "df = pd.DataFrame({\n",
    "    'entity': [item[0] for item in ordered_bpe_entities],\n",
    "    'index': list(range(len(ordered_bpe_entities)))\n",
    "})\n",
    "\n",
    "# Print first 20 entities with their indices\n",
    "print(\"\\nFirst 20 entities:\")\n",
    "print(df.head(20))\n",
    "\n",
    "# Search for specific entities\n",
    "def search_entity(name):\n",
    "    if name in entity_to_idx:\n",
    "        idx = entity_to_idx[name]\n",
    "        print(f\"Entity '{name}' found at index {idx}\")\n",
    "        print(f\"BPE tokens: {ordered_bpe_entities[idx][1]}\")\n",
    "        print(f\"Shaped BPE: {ordered_bpe_entities[idx][2]}\")\n",
    "    else:\n",
    "        print(f\"Entity '{name}' not found in the mapping\")\n",
    "\n",
    "# Check for specific entities related to your error\n",
    "print(\"\\nSearching for specific entities:\")\n",
    "search_entity(\"concept_date_n2000\")  # The one causing the KeyError\n",
    "search_entity(\"concept_profession_associate\")  # From your first triple\n",
    "\n",
    "# Save all entities to a CSV for easier inspection\n",
    "df.to_csv(\"all_entities.csv\", index=False)\n",
    "print(\"\\nAll entities saved to 'all_entities.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "363be175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entities: 22411\n",
      "Data type: <class 'list'>\n",
      "\n",
      "First 20 entities:\n",
      "    entity  index\n",
      "0               0\n",
      "1               1\n",
      "2               2\n",
      "3               3\n",
      "4               4\n",
      "..     ...    ...\n",
      "266           266\n",
      "267           267\n",
      "268           268\n",
      "269           269\n",
      "270           270\n",
      "\n",
      "[271 rows x 2 columns]\n",
      "\n",
      "Searching for specific entities:\n",
      "Entity 'concept_company_maxis' not found in the mapping\n",
      "Entity 'concept_profession_associate' not found in the mapping\n",
      "\n",
      "All entities saved to 'all_entities.csv'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Path to the file\n",
    "file_path = r\"C:\\Users\\Harshit Purohit\\Byte\\myenv7\\Lib\\site-packages\\dicee\\Experiments\\2025-08-23 14-35-15.926097\\ordered_bpe_entities.p\"\n",
    "\n",
    "# Open the file in binary read mode\n",
    "with open(file_path, 'rb') as f:\n",
    "    # Load the pickle data\n",
    "    ordered_bpe_entities = pickle.load(f)\n",
    "\n",
    "# Basic information\n",
    "print(f\"Total entities: {len(ordered_bpe_entities)}\")\n",
    "print(f\"Data type: {type(ordered_bpe_entities)}\")\n",
    "\n",
    "# Create a mapping dictionary (entity string → index)\n",
    "entity_to_idx = {item[0]: idx for idx, item in enumerate(ordered_bpe_entities)}\n",
    "\n",
    "# Create a DataFrame for easier analysis\n",
    "df = pd.DataFrame({\n",
    "    'entity': [item[0] for item in ordered_bpe_entities],\n",
    "    'index': list(range(len(ordered_bpe_entities)))\n",
    "})\n",
    "\n",
    "# Print first 20 entities with their indices\n",
    "print(\"\\nFirst 20 entities:\")\n",
    "print(df.head(271))\n",
    "\n",
    "# Search for specific entities\n",
    "def search_entity(name):\n",
    "    if name in entity_to_idx:\n",
    "        idx = entity_to_idx[name]\n",
    "        print(f\"Entity '{name}' found at index {idx}\")\n",
    "        print(f\"BPE tokens: {ordered_bpe_entities[idx][1]}\")\n",
    "        print(f\"Shaped BPE: {ordered_bpe_entities[idx][2]}\")\n",
    "    else:\n",
    "        print(f\"Entity '{name}' not found in the mapping\")\n",
    "\n",
    "# Check for specific entities related to your error\n",
    "print(\"\\nSearching for specific entities:\")\n",
    "search_entity(\"concept_company_maxis\")  # The one causing the KeyError\n",
    "search_entity(\"concept_profession_associate\")  # From your first triple\n",
    "\n",
    "# Save all entities to a CSV for easier inspection\n",
    "df.to_csv(\"all_entities.csv\", index=False)\n",
    "print(\"\\nAll entities saved to 'all_entities.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c392fb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 'concept_company_maxis' not found in the mapping\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Path to the file\n",
    "file_path = r\"C:\\Users\\Harshit Purohit\\Byte\\myenv7\\Lib\\site-packages\\dicee\\Experiments\\2025-08-23 14-35-15.926097\\ordered_bpe_entities.p\"\n",
    "\n",
    "# Open the file in binary read mode\n",
    "with open(file_path, 'rb') as f:\n",
    "    # Load the pickle data\n",
    "    ordered_bpe_entities = pickle.load(f)\n",
    "\n",
    "# Create entity to index mapping\n",
    "entity_to_idx = {item[0]: idx for idx, item in enumerate(ordered_bpe_entities)}\n",
    "\n",
    "# Search for the specific entity\n",
    "target = \"concept_company_maxis\"\n",
    "if target in entity_to_idx:\n",
    "    idx = entity_to_idx[target]\n",
    "    print(f\"Found '{target}' at index {idx}\")\n",
    "    print(f\"BPE tokens: {ordered_bpe_entities[idx][1]}\")\n",
    "    print(f\"Shaped BPE: {ordered_bpe_entities[idx][2]}\")\n",
    "else:\n",
    "    print(f\"Entity '{target}' not found in the mapping\")\n",
    "    \n",
    "    # Check if any similar entities exist (partial match)\n",
    "    similar = [ent for ent in entity_to_idx.keys() if \"concept_date\" in ent]\n",
    "    if similar:\n",
    "        print(f\"Found {len(similar)} similar entities:\")\n",
    "        for ent in similar[:10]:  # Show first 10\n",
    "            print(f\"  - {ent}\")\n",
    "        if len(similar) > 10:\n",
    "            print(f\"  ... and {len(similar)-10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08ee7eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Best Model per Dataset ---\n",
      "Dataset: KGs/UMLS\n",
      "  Model: QMult\n",
      "  Learning Rate: 0.001\n",
      "  Embed Dim: 64\n",
      "  Batch Size: 512\n",
      "  Train MRR: 0.9767\n",
      "  Test MRR: 0.8763\n"
     ]
    }
   ],
   "source": [
    "# Simple Jupyter cell: load a grid‐search .txt and print best model per dataset\n",
    "\n",
    "txt_path = r\"C:\\Users\\Harshit Purohit\\OneDrive\\Desktop\\KG_Work\\Results\\Important\\UMLS\\UMLS_CT2_TF.txt\"  # ← update this to your file path\n",
    "\n",
    "import csv\n",
    "\n",
    "best_results = {}\n",
    "with open(txt_path, newline='') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        ds = row['dataset']\n",
    "        te = float(row['test_mrr'])\n",
    "        if ds not in best_results or te > best_results[ds]['test_mrr']:\n",
    "            best_results[ds] = {\n",
    "                'model': row['model'],\n",
    "                'lr': row['lr'],\n",
    "                'embed_dim': row['embed_dim'],\n",
    "                'batch_size': row['batch_size'],\n",
    "                'train_mrr': float(row['train_mrr']),\n",
    "                'test_mrr': te\n",
    "            }\n",
    "\n",
    "print(\"--- Best Model per Dataset ---\")\n",
    "for ds, b in best_results.items():\n",
    "    print(f\"Dataset: {ds}\")\n",
    "    print(f\"  Model: {b['model']}\")\n",
    "    print(f\"  Learning Rate: {b['lr']}\")\n",
    "    print(f\"  Embed Dim: {b['embed_dim']}\")\n",
    "    print(f\"  Batch Size: {b['batch_size']}\")\n",
    "    print(f\"  Train MRR: {b['train_mrr']:.4f}\")\n",
    "    print(f\"  Test MRR: {b['test_mrr']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79662f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Best Test MRR per Model ===\n",
      "Model:      DistMult\n",
      "  Dataset:   KGs/UMLS\n",
      "  LR:        0.001\n",
      "  Embed Dim: 32\n",
      "  Batch Size: 512\n",
      "  Train MRR: 0.8315\n",
      "  Test MRR:  0.7363\n",
      "\n",
      "Model:      ComplEx\n",
      "  Dataset:   KGs/UMLS\n",
      "  LR:        0.001\n",
      "  Embed Dim: 64\n",
      "  Batch Size: 512\n",
      "  Train MRR: 0.9267\n",
      "  Test MRR:  0.8628\n",
      "\n",
      "Model:      QMult\n",
      "  Dataset:   KGs/UMLS\n",
      "  LR:        0.001\n",
      "  Embed Dim: 64\n",
      "  Batch Size: 512\n",
      "  Train MRR: 0.9767\n",
      "  Test MRR:  0.8763\n",
      "\n",
      "Model:      Keci\n",
      "  Dataset:   KGs/UMLS\n",
      "  LR:        0.001\n",
      "  Embed Dim: 64\n",
      "  Batch Size: 512\n",
      "  Train MRR: 0.9467\n",
      "  Test MRR:  0.8735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Jupyter cell: select best Test MRR per model from your .txt\n",
    "\n",
    "txt_path = r\"C:\\Users\\Harshit Purohit\\OneDrive\\Desktop\\KG_Work\\Results\\Important\\UMLS\\UMLS_CT2_TF.txt\"  # ← update this to your file path\n",
    "\n",
    "import csv\n",
    "\n",
    "best_per_model = {}\n",
    "with open(txt_path, newline='') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        model = row['model']\n",
    "        test_mrr = float(row['test_mrr'])\n",
    "        # keep the row if this model is new or has a higher Test MRR than before\n",
    "        if model not in best_per_model or test_mrr > best_per_model[model]['test_mrr']:\n",
    "            best_per_model[model] = {\n",
    "                'dataset':    row['dataset'],\n",
    "                'lr':         row['lr'],\n",
    "                'embed_dim':  row['embed_dim'],\n",
    "                'batch_size': row['batch_size'],\n",
    "                'train_mrr':  float(row['train_mrr']),\n",
    "                'test_mrr':   test_mrr\n",
    "            }\n",
    "\n",
    "print(\"=== Best Test MRR per Model ===\")\n",
    "for m, info in best_per_model.items():\n",
    "    print(f\"Model:      {m}\")\n",
    "    print(f\"  Dataset:   {info['dataset']}\")\n",
    "    print(f\"  LR:        {info['lr']}\")\n",
    "    print(f\"  Embed Dim: {info['embed_dim']}\")\n",
    "    print(f\"  Batch Size: {info['batch_size']}\")\n",
    "    print(f\"  Train MRR: {info['train_mrr']:.4f}\")\n",
    "    print(f\"  Test MRR:  {info['test_mrr']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459791ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Best Test MRR per Model ===\n",
      "      model            dataset     lr  embed_dim  batch_size  train_mrr  \\\n",
      "0   ComplEx  KGs/NELL-995-h100  0.001         32         512   0.515648   \n",
      "1  DistMult  KGs/NELL-995-h100  0.001         64         512   0.657048   \n",
      "2      Keci  KGs/NELL-995-h100  0.001         64         512   0.681594   \n",
      "3     QMult  KGs/NELL-995-h100  0.001         64         512   0.740479   \n",
      "\n",
      "   test_mrr  \n",
      "0  0.265026  \n",
      "1  0.271156  \n",
      "2  0.278941  \n",
      "3  0.283503  \n"
     ]
    }
   ],
   "source": [
    "# Jupyter cell: compute best Test MRR per model from your per‐run .txt files\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "# point this to your results folder\n",
    "eval_dir = r\"C:\\Users\\Harshit Purohit\\Downloads\\Final_Eval_NELL_h100_CT\"\n",
    "\n",
    "# collect all runs\n",
    "records = []\n",
    "for fn in glob.glob(os.path.join(eval_dir, \"md*.txt\")):\n",
    "    with open(fn, \"r\") as f:\n",
    "        entry = {}\n",
    "        for line in f:\n",
    "            key, _, val = line.partition(\":\")\n",
    "            v = val.strip()\n",
    "            if key == \"Model\":\n",
    "                entry[\"model\"] = v\n",
    "            elif key == \"Dataset\":\n",
    "                entry[\"dataset\"] = v\n",
    "            elif key == \"Learning Rate\":\n",
    "                entry[\"lr\"] = float(v)\n",
    "            elif key == \"Embed Dim\":\n",
    "                entry[\"embed_dim\"] = int(v)\n",
    "            elif key == \"Batch Size\":\n",
    "                entry[\"batch_size\"] = int(v)\n",
    "            elif key == \"Train MRR\":\n",
    "                entry[\"train_mrr\"] = float(v)\n",
    "            elif key.startswith(\"Test\"):\n",
    "                entry[\"test_mrr\"] = float(v)\n",
    "        if entry:\n",
    "            records.append(entry)\n",
    "\n",
    "# build a DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# fix: group by the lowercase column name \"model\"\n",
    "best_per_model = df.loc[df.groupby(\"model\")[\"test_mrr\"].idxmax()] \\\n",
    "                   .reset_index(drop=True)\n",
    "\n",
    "print(\"=== Best Test MRR per Model ===\")\n",
    "print(best_per_model[[\"model\",\"dataset\",\"lr\",\"embed_dim\",\"batch_size\",\"train_mrr\",\"test_mrr\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "378f0bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04760771 → materiality.n.02\n",
      "05074057 → straightness.n.02\n",
      "08390511 → militia.n.01\n",
      "02045024 → alcidae.n.01\n",
      "01257145 → precession.n.02\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# from nltk.corpus import wordnet as wn\n",
    "# print(wn.get_version())\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def synset_from_offset(offset):\n",
    "    # offsets in WN18RR omit the POS; try all four\n",
    "    off = int(offset)  # strips leading zeros\n",
    "    for pos in ['n','v','a','r']:\n",
    "        try:\n",
    "            ss = wn.synset_from_pos_and_offset(pos, off)\n",
    "            return ss.name()\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "for o in [\"04760771\",\"05074057\",\"08390511\",\"02045024\",\"01257145\"]:\n",
    "    print(o, \"→\", synset_from_offset(o))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27c5b863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: tensor([[[-1.8543, -1.0132, -0.1687,  0.5912, -1.4759,  2.2215, -0.8551,\n",
      "          -0.2148],\n",
      "         [ 1.2463,  0.4304, -2.3139,  0.3637, -1.8702,  0.6693,  0.2372,\n",
      "          -0.5150],\n",
      "         [ 0.7012,  1.5165,  0.3433, -0.1700, -0.6118, -0.4939, -0.9294,\n",
      "           0.1986],\n",
      "         [ 1.4799, -1.6772,  0.8918,  0.5247,  0.5485, -0.1123, -1.1373,\n",
      "           0.4005]]], grad_fn=<EmbeddingBackward0>)\n",
      "#################\n",
      "Transformer output:\n",
      "tensor([[[-0.1439, -1.1382,  0.2441,  0.8766, -2.0264,  1.0427,  0.6303,\n",
      "           0.5148]],\n",
      "\n",
      "        [[ 1.2576,  0.8482, -1.0732, -0.1664, -0.8887, -1.3009, -0.1007,\n",
      "           1.4238]],\n",
      "\n",
      "        [[ 0.9872,  1.2110,  1.0316, -0.6906, -1.4883, -0.9316, -0.7408,\n",
      "           0.6214]],\n",
      "\n",
      "        [[ 0.8380, -1.7721,  0.8079,  0.9603,  0.2592, -1.5147, -0.0464,\n",
      "           0.4679]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harshit Purohit\\Byte\\myenv7\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Define embedding parameters\n",
    "embedding_dim = 8  # Size of each embedding vector\n",
    "vocab_size = 10    # Number of unique tokens in the vocabulary\n",
    "\n",
    "# Create an embedding layer\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# Example input: indices of words in the vocabulary\n",
    "input_indices = torch.tensor([[1, 3, 5, 7]])  # Shape: (batch_size, sequence_length)\n",
    "\n",
    "# Get the embeddings for the input indices\n",
    "embeddings = embedding_layer(input_indices)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "print(f\"Embeddings: {embeddings}\")\n",
    "\n",
    "# Transpose embeddings to match Transformer input shape\n",
    "embeddings = embeddings.permute(1, 0, 2)  # Shape: (sequence_length, batch_size, embedding_dim)\n",
    "\n",
    "print(\"#################\")\n",
    "\n",
    "# Define a TransformerEncoderLayer\n",
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=embedding_dim,\n",
    "    nhead=2  # Number of attention heads\n",
    ")\n",
    "\n",
    "# Define a TransformerEncoder with multiple layers\n",
    "transformer_encoder = nn.TransformerEncoder(\n",
    "    encoder_layer=encoder_layer,\n",
    "    num_layers=4  # Number of encoder layers\n",
    ")\n",
    "\n",
    "# Pass the embeddings through the Transformer encoder\n",
    "output = transformer_encoder(embeddings)\n",
    "\n",
    "print(\"Transformer output:\")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
